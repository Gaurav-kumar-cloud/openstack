###########Ceph insallation on single node###########

#######Netplan configuration#########
vi /etc/netplan/00-installer-config.yaml

 # This is the network config written by 'subiquity'
network:
  ethernets:
    eno3:
      dhcp4: false
      dhcp6: false
      addresses:
      - 102.223.185.34/27
      - 10.10.12.40/24
      match:
        macaddress: 18:66:DA:8C:B6:5F
      nameservers:
        addresses:
        - 8.8.8.8
        - 8.8.4.4
      routes:
      - to: default
        via: 102.223.185.33
    eno4:
      dhcp4: false
      dhcp6: false
      match:
        macaddress: 18:66:DA:8C:B6:61
  version: 2
  
##############Update & upgrade the system###########
apt -y update && apt -y upgrade

###########Edit the /etc/hosts file:##########
<102.223.185.34>  <seemoosp>

#########Install docker and its dependencies##############
sudo apt install apt-transport-https ca-certificates curl software-properties-common -y
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt -y update
sudo apt -y install docker-ce
sudo systemctl enable --now docker

########Install cephadm########
apt install cephadm -y

###### Add Ceph Repository######### 
Add the official Ceph repository for the desired release (e.g., reef):

cephadm add-repo --release reef

##########Install Ceph Packages#############
cephadm install

############Bootstrap the Ceph Cluster##########
Initialize the Ceph cluster and deploy the first monitor with dashboard access:

cephadm bootstrap --mon-ip 102.223.185.34 --initial-dashboard-user Admin --initial-dashboard-password Admin098! --dashboard-password-noupdate


##########Install Common Ceph Tools############
cephadm install ceph-common

#############################Set public network for our ceph cluster ############
sudo ceph config set mon public_network 102.223.185.0/27

##################Deploy monitors on a specific set of hosts ###########
ceph orch host ls
sudo ceph orch apply mon "seemoosp"

########### ########## ceph add lable ############
 sudo ceph orch host label add seemoosp mon
 
######################### create osd from a specific device on a specific host #############
vi seemoosp_osd.yml
service_type: osd
service_id: seemoosp_osd
service_name: osd.seemoosp_osd
placement:
  host_pattern: "seemoosp"
spec:
  data_devices:
    paths:
    - /dev/sda
    - /dev/sdb
    - /dev/sdc
  unmanaged: False
  
ceph orch apply osd -i seemoosp_osd.yml

################## DEPLOY RGWS service #####################
ceph orch host label add seemoosp rgw
ceph orch apply rgw radosGW '--placement=label:rgw count-per-host:1' --port=8080 

############## rgw user list and info  ####### 
radosgw-admin user list
radosgw-admin user info --uid=dashboard

################mgr set up on node##########
ceph orch apply mgr --placement=" seemoosp "
ceph balancer on

#############install chrony and s3cmd on node############
apt install -y chrony && apt install -y s3cmd

#########command to set the osd pool size replica1  #########
for pool in $(ceph osd pool ls); do
  ceph osd pool set "$pool" size 2
done

##########Configure s3cmd########
vim /root/.s3cfg

[default]
access_key = BUE9HCCL05BCXLL6BA90
secret_key = kw6dm2WAyLwJED7k9kzyzDpNVrf8ltSydug7Ue0I

host_base = <http://192.168.122.141:8080>
host_bucket = <http://192.168.122.141:8080>
use_https = False

######Verify S3cmd Configuration###########
Create a New Bucket

s3cmd mb s3://rah010
s3cmd ls



####################Integration steps for ceph with openstack#############
###Create OSD Pools###
 
ceph osd pool create volumes
ceph osd pool create images
ceph osd pool create vms
ceph osd pool create gnocchi
ceph osd pool create backup

ceph osd pool ls

####Initialize the RBD Pools####
rbd pool init volumes
rbd pool init images
rbd pool init vms
rbd pool init gnocchi
rbd pool init backup


######Ceph Keyring Creation for OpenStack Integration##########
###Create Keyring for Glance###
ceph auth get-or-create client.glance mon 'profile rbd' osd 'profile rbd pool=images' mgr 'profile rbd pool=images' > /root/ceph.client.glance.keyring

###Create Keyring for Cinder###
ceph auth get-or-create client.cinder mon 'profile rbd' osd 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd-read-only pool=images' mgr 'profile rbd pool=volumes, profile rbd pool=vms' > /root/ceph.client.cinder.keyring

###Create Keyring for Cinder Backup###
ceph auth get-or-create client.cinder-backup mon 'profile rbd' osd 'profile rbd pool=backups' mgr 'profile rbd pool=backups' > /root/ceph.client.cinder-backup.keyring

###Create Keyring for Gnocchi###
ceph auth get-or-create client.gnocchi mon 'profile rbd' osd 'profile rbd pool=gnocchi' mgr 'profile rbd pool=gnocchi' > /root/ceph.client.gnocchi.keyring

###Create Keyring for Manila (Shared File System Service)###
ceph auth add client.manila mon 'profile rbd' osd 'profile rbd pool=gnocchi' mgr 'profile rbd pool=gnocchi' > /root/ceph.client.gnocchi.keyring

###Create Keyring for Nova###
ceph auth get-or-create client.nova mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=vms, allow rx pool=images' -o /root/ceph.client.nova.keyring

#### Set Ceph Capabilities for Manila#####
ceph auth caps client.manila osd 'allow rw pool=cephfs_data' mgr 'allow rw' mon 'allow r'

####Retrieve the Manila Keyring####
ceph auth get client.manila -o /root/manila.keyring



#################Openstack deployment###############
###System Update###
sudo apt update -y && sudo apt upgrade -y

####Install System Dependencies (Non-Virtual Environment)###
sudo apt install git python3-dev libffi-dev gcc libssl-dev -y

###Install Python Virtual Environment Tools###
sudo apt install python3-venv -y

##Create and activate a Python virtual environment:##
python3 -m venv /path/to/venv
source /path/to/venv/bin/activate
pip install -U pip

### Install Ansible and Kolla-Ansible###
pip install 'ansible-core>=2.15,<2.16'

###Install Kolla-Ansible from Source###
pip install git+https://opendev.org/openstack/kolla-ansible@stable/2024.1

###Create and set permissions on the Kolla configuration directory:##
sudo mkdir -p /etc/kolla
sudo chown $USER:$USER /etc/kolla

###Copy Kolla-Ansible configuration examples:###
cp -r /path/to/venv/share/kolla-ansible/etc_examples/kolla/* /etc/kolla
cp /path/to/venv/share/kolla-ansible/ansible/inventory/* .

####Install Ansible Galaxy Requirements####
kolla-ansible install-deps

############# Generate Kolla passwords #####
kolla-genpwd

##########edit /etc/kolla/globals.yaml file############
---
################Services enabled by default############
#Keystone, Glance, Nova, Placement, Neutron, Cinder, Heat, Horizon. 

kolla_base_distro: "ubuntu"
kolla_install_type: "binary"
enable_haproxy: "yes"
keepalived_virtual_router_id: "51"
enable_chrony: "yes"
#enable_etcd: "yes"
config_strategy: "COPY_ALWAYS"

#################################### Monitoring Services
enable_prometheus: "yes"
enable_grafana: "yes"
prometheus_node_exporter_port: "9101"
grafana_server_port: "3001"
prometheus_alertmanager_port: "9097"
prometheus_alertmanager_cluster_port: "9096"
prometheus_haproxy_exporter_port: "9111"


###################### neutron Services ###########
kolla_external_vip_address: "102.223.185.50"  # External VIP
kolla_internal_vip_address: "102.223.185.51"  # Internal VIP
api_interface: "eno3"
network_interface: "eno3"
neutron_external_interface: "eno4"
neutron_bridge_name: "br-ex"
neutron_plugin_agent: "openvswitch"
neutron_tenant_network_types: "vxlan,vlan,flat"
enable_neutron_provider_networks: "yes"
enable_neutron_qos: "yes"
enable_neutron_vpnaas: "yes"
enable_neutron_trunk: "yes"
#enable_neutron_metering: "yes"
enable_neutron_port_forwarding: "yes"
#enable_neutron_agent_ha: "Yes" ################### HA for neutron agent


######################################################## Image Services ######
glance_backend_ceph: "yes"
ceph_glance_keyring: "client.glance.keyring"
ceph_glance_user: "glance"
ceph_glance_pool_name: "images"
enable_glance_image_cache: "yes"

################################################### Nova #############
nova_compute_virt_type: "kvm"
nova_backend_ceph: "yes"
ceph_nova_keyring: "client.cinder.keyring"
ceph_nova_user: "cinder"
ceph_nova_pool_name: "vms"
nova_console: "novnc"

############################# Cinder services  ################

################### Cinder_volume in ceph ###
enable_hacluster: "yes"
enable_cinder: "yes"
cinder_backend_ceph: "yes"
ceph_cinder_keyring: "client.cinder.keyring"
ceph_cinder_user: "cinder"
ceph_cinder_pool_name: "volumes"
############################# Cinder_backup in ceph ###
enable_cinder_backup: "yes"
ceph_cinder_backup_keyring: "client.cinder-backup.keyring"
ceph_cinder_backup_user: "cinder-backup"
ceph_cinder_backup_pool_name: "backups"

######################################### Metric #####
#enable_ceilometer: "yes"
#enable_gnocchi: "yes"
#gnocchi_backend_storage: "ceph"
#ceph_gnocchi_keyring: "client.gnocchi.keyring"

##############enable redis######
enable_redis: "yes"
##########Deploy masakari#####
#enable_masakari: "yes"
###################enable_barbican#######
enable_barbican: "yes"

###############magnum
enable_magnum: "yes"
enable_cluster_user_trust: True
enable_octavia: "yes"
octavia_auto_configure: "no"

##################Swift and S3 options ###############
enable_swift_s3api: "yes"
enable_ceph_rgw: true
enable_swift: "no"
enable_ceph_rgw_keystone: true
ceph_rgw_swift_compatibility: "false"
ceph_rgw_swift_account_in_url: "false"
ceph_rgw_hosts:
  - host: seemoosp
    ip: 102.223.185.34
    port: 8080

################## TlS
kolla_enable_tls_external: "yes"
kolla_enable_tls_internal: "no"
kolla_enable_tls_backend: "no"
kolla_verify_tls_backend: "no"
kolla_copy_ca_into_containers: "no"
kolla_external_fqdn: d1-cloud.dodil.io
acme_client_servers:
  - server certbot 10.10.12.40:80
kolla_external_fqdn_cert: "/etc/kolla/certificates/haproxy.pem"


#########Create dicrectories for storing ceph keyring to integrate with openstack#########
mkdir -p /etc/kolla/config/{nova,cinder,glance,gnocchi}
mkdir /etc/kolla/config/cinder/cinder-volume/
mkdir /etc/kolla/config/cinder/cinder-backup/
tree /etc/kolla/config/
/etc/kolla/config/
├── cinder
│ ├── cinder-backup
│ │ ├── ceph.client.cinder-backup.keyring
│ │ ├── ceph.client.cinder.keyring
│ │ └── ceph.conf
│ └── cinder-volume
│ ├── ceph.client.cinder.keyring
│ └── ceph.conf
├── cinder.conf
├── freezer.conf
├── glance
│ ├── ceph.client.glance.keyring
│ └── ceph.conf
├── glance.conf
├── horizon
│ └── custom_local_settings
└── nova
├── ceph.client.cinder.keyring
    ceph.conf

########Bootstrap Target Servers#######
Use the bootstrap-servers command to install dependencies and prepare the target host(s):

kolla-ansible -i ./all-in-one bootstrap-servers

##########Run Prechecks##########
Before deploying OpenStack, run the pre-deployment checks to verify environment readiness:

kolla-ansible -i ./all-in-one prechecks

#########Deploy OpenStack############
Proceed with the full deployment of OpenStack services:

kolla-ansible -i ./all-in-one deploy


######Post-Deployment Steps for Kolla-Ansible######
Install OpenStack Client
pip install python-openstackclient -c https://releases.openstack.org/constraints/upper/master

#########Run Post-Deploy Configuration##########
Execute the post-deploy command to complete the Kolla-Ansible deployment. This step generates the admin-openrc.sh file, sets up initial OpenStack services, and configures access credentials:

kolla-ansible post-deploy

##########Now can access the dashboard########
http://102.223.185.50
username = admin
password = /etc/kolla/admin-openrc.sh   (here you can get password)

