############Kubernetes##################
#########What is Docker?#################
  Docker is a containerization platform. It packages applications and their dependencies into containers—lightweight, portable units of software that ensure consistency across environments (development, testing, production, etc.).
  
   ==>why lightwieght?
        Containers are lightweight because they share the host operating system's kernel instead of running their own full OS like virtual machines (VMs). Let’s break this down clearly:
        Why Containers Are Lightweight

1. No Separate OS
    VMs run a full guest OS (like Ubuntu, CentOS) for each instance.
    Containers run only the app and its dependencies, sharing the host OS kernel (e.g., Linux).

  #That means:
    Less CPU and memory overhead.
    Faster startup (seconds instead of minutes).
    Smaller size (MBs vs. GBs for VMs).

2. Uses OS Features: Namespaces & cgroups
 =>Namespaces
    Isolate resources (processes, file systems, network, etc.) for each container.
    It feels like a separate machine to the container—but it's not!

   Control Groups (cgroups)
    Limit and monitor CPU, memory, disk I/O for containers.
    Efficiently manage multiple containers.

These features are built into the Linux kernel, so Docker just uses them—it doesn't need to "reinvent the OS".

3. Layered File System (UnionFS)
    Docker images are built in layers.
    Reuses common layers across containers.
    Saves disk space and speeds up builds.

   Example:
    Ubuntu base image: 60MB
    NGINX layer: 30MB
    App layer: 20MB
    ➡ Total container size = not 110MB, because it reuses the base layers.

4. Faster Boot Time
    Containers start in less than a second, because:
        No OS boot needed.
        Just launching a process inside the isolated environment.

5. Minimal Dependencies
    A container includes only what it needs to run (no unnecessary system packages).
    This makes it small and efficient.
    
    
########why containers are epherimal in nature?###############
  ==>What Does "Ephemeral" Mean?
     Ephemeral = short-lived, non-persistent, and disposable.

In the context of containers, it means:
    A container can be created, stopped, restarted, or destroyed at any time.
    Any data stored inside the container filesystem (like /var, /home) is lost when the container stops or crashes—unless it’s stored on a volume.

==>Why Containers Are Ephemeral
1. ✅ Designed for Scalability and Automation
    In Kubernetes or Docker Swarm, containers are often automatically killed and recreated (e.g., during scaling, updates, failures).
    If containers held important data, you'd risk losing it during those operations.

2. Stateless Architecture
    Containers are built to run stateless services (like web servers, APIs).
    The state (data, session info) should be stored in:
        External databases (e.g., MySQL, MongoDB)
        Object storage (e.g., S3, Ceph)
        Docker volumes or persistent storage in Kubernetes


###Why We Switched from VMs to Containers
1. Efficiency and Performance
    VMs: Each virtual machine includes a full OS, its own kernel, and all the dependencies.
        This means high resource consumption (CPU, RAM, disk).

    Containers: Share the host OS kernel and are much lighter (just the app + its dependencies).
        Much faster to start, use less RAM and CPU.

#Example:
Running 10 microservices:
    On VMs: Might need 10 GB RAM and 10 CPUs.
    On Containers: Might only need 2 GB RAM and 2 CPUs.

2. Faster Start-up Times
    VMs take minutes to boot (load OS, services).
    
    Containers start in seconds or milliseconds, since there’s no full OS boot process.

#Example:
    Spinning up a VM on VirtualBox vs. a Docker container:
        VM: 30–60 seconds.
        Docker: Less than 1 second.

3. Better Resource Utilization
    Containers can pack many applications tightly onto a host, reducing waste.

    VMs often run idle processes or multiple copies of OS services.

#Real-world benefit:
    A Kubernetes cluster with 20 containers can run on 4 GB RAM.
    But with 20 VMs, you might need 20+ GB RAM.

4. Portability
    Containers work the same way on:
        Developer laptops
        Testing environments
        Production clusters

This is often summarized as:
    “It works on my machine” problem gets solved with containers.

#Example:
A Node.js app in a Docker container can run on:
    Linux, Windows, macOS
    On-prem servers, AWS, GCP, Azure — without modification.

5. DevOps and CI/CD Integration
    Containers are perfect for automation:
        Easy to create disposable environments
        Fast to tear down and rebuild

#Example:
    Jenkins pipelines can build, test, and deploy containers automatically within seconds.
    With VMs, provisioning and configuration is much slower.

6. Isolation and Security
    VMs offer stronger isolation via hypervisors, but are heavier.

    Containers offer process-level isolation, which is lighter and good enough for many apps.
    With tools like Kubernetes, AppArmor, and Seccomp, container security is improving fast.


###When to Use What?
Use VMs when:
    You need strong isolation (multi-tenant apps with sensitive data).
    You're running legacy applications that require a full OS.
    You want to run different OS types (e.g., Linux + Windows).

Use Containers when:
    You need fast scalability and resource efficiency.
    You're developing microservices.
    You're building cloud-native apps.
    You want portable, consistent environments from dev to prod.



####What Are Microservices?
Microservices (short for microservice architecture) is a way of designing software where an application is built as a collection of small, independent services, each focused on a specific business function and communicating with each other — usually over HTTP APIs or messaging systems.

##How Microservices Communicate
    Usually via:
        REST APIs (HTTP)
        gRPC (faster, typed)
        Message Brokers (RabbitMQ, Kafka)

###Microservices vs Monolith
Feature     	    Monolith (Old Way)	                         Microservices (Modern Way)
Structure	    One big codebase	                           Many small, focused services
Deployment	    Single deploy for whole app	                   Deploy services independently
Scalability	    Whole app must scale together	           Scale only what’s needed
Fault Isolation	    One bug can crash entire system	           One service crash doesn't break others
Tech Flexibilit     One language/stack only	                   Each service can use different tech

##Simple Analogy##
    Think of a monolithic app like a restaurant where the chef does everything — cooking, cleaning, serving.
    A microservices app is like a team of specialists — chef, waiter, cleaner, cashier — each focusing on one task efficiently.


############What is Kubernetes?################
Kubernetes (K8s) is an open-source container orchestration platform. It automates the deployment, scaling, and management of containerized applications.
Kubernetes was originally developed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF).    

  ==>Why Kubernetes Exists
       Docker lets you run a single container, or maybe a few using docker-compose. But in real-world production, you need:
         100s or 1000s of containers
         Running on multiple machines
         With high availability, auto-scaling, load balancing, and self-healing

💥 Kubernetes solves this complexity.
##Kubernetes automates:                                                                   
Task	                     What It Does
Deployment	        Deploy multiple containers across clusters
Scaling	                Automatically increases/decreases number of containers
Load Balancing	        Distributes traffic across containers
Self-healing	        Restarts failed containers automatically
Service Discovery	Lets containers find and talk to each other
Rollouts/Rollbacks	Gradually updates apps with zero downtime
Storage Orchestration	Mounts local or cloud storage dynamically

##Kubernetes vs Docker
Feature	                                Docker	                                          Kubernetes
Purpose	                         Container runtime	                         Container orchestration
Manages Containers?	               Yes	                                   Yes, at large scale
Multi-node support	              Manual	                                        Built-in
Self-healing	                         No	                                   Yes (restarts, replaces failed pods)
Load Balancing	                      Basic (manual)	                                  Automatic
Auto-scaling	                         No	                                           Yes
Storage Integration	                Basic	                             Dynamic provisioning
Networking	                        Simple	                          Advanced, includes service discovery


##Docker vs Docker Swarm vs Kubernetes
Feature        	Docker	                 Docker Swarm	                     Kubernetes
Level	  Container runtime	    Basic orchestration	                Full orchestration
Scalability	Low	                Medium	                            Very High
Community	Large	                Smaller	                       Very large & active
Used for	Small deployments	Mid-sized clusters	          Large-scale systems


##problems with docker###

prob-1)
Ephemeral = Short Life
    Containers are ephemeral, meaning they are temporary and can die (stop) and revive (restart) at any time.
    This is great for scalability, but it introduces challenges in managing stability, persistence, and scaling.

🔹 Memory Allocation
    The diagram shows a memory block with containers running inside (like a 100-slot memory block).
    As containers are created and destroyed:
        Some memory (slots) are used temporarily.
        Some containers die (marked with ✗).
        They don't automatically restart unless explicitly configured.
        This leads to resource fragmentation and service unavailability.

**🔹Single Host Limitation
      Docker by default operates on a single host.
      This limits scalability, availability, and resource utilization.

prob-2)Auto-healing:
prob-3)Auto-scaling:
prob-4)simple platform:
        
        
###Kubernetes Architecture Diagram (Layout)
Here is a visual layout of the architecture:

+-------------------------------------------------------------+
|                      Kubernetes Control Plane               |
|                                                             |
|  +----------------+  +----------------------+               |
|  | etcd (DB)      |  | kube-apiserver       |<-- kubectl    |
|  +----------------+  +----------------------+               |
|                             |                                |
|  +--------------------------+------------------------------+ |
|  |  kube-scheduler        |    kube-controller-manager     | |
|  +------------------------+-------------------------------+ |
|                                                             |
+---------------------^------------------------------^--------+
                      |                              |
                      | Schedules                    | Desired state
                      v                              v
+----------------------------+     +----------------------------+
| Worker Node 1/Data Plane         |      Worker Node 2         |
|                            |     |                            |
|  +--------+   +----------+ |     |  +--------+   +----------+ |
|  | kubelet|   |kube-proxy| |     |  | kubelet|   |kube-proxy| |
|  +--------+   +----------+ |     |  +--------+   +----------+ |
|       |                    |     |       |                    |
|       v                    |     |       v                    |
|  +-----------+             |     |  +-----------+             |
|  |  Pod(s)   | (container) |     |  |  Pod(s)   | (container) |
|  +-----------+             |     |  +-----------+             |
+----------------------------+     +----------------------------+
|               <------- Container Network ---------->
|________________________________________________________________|                
                
  
  
##Docker Architecture Components
Component	                                  Description
Docker Engine	                            Core that runs containers
Docker CLI	                            Command-line interface (docker run, docker build, etc.)
Docker Daemon (dockerd)	                    Background service that manages containers/images
Docker Images	                            Read-only templates used to create containers
Docker Containers	                    Running instances of Docker images
Dockerfile	                            Instructions to build Docker images
Docker Hub	                            Public registry to store and share images    


##Docker Engine Flow (Step-by-Step)
Let’s say you run:

docker run nginx

#Here’s what happens:
 -->Docker CLI sends request to Docker Daemon (dockerd)
    
    Docker daemon:
        Checks if nginx image is present locally
        If not → pulls it from Docker Hub

 -->Docker uses container runtime (containerd) to:
        Unpack the image (layers of file system)
        Set up namespaces, cgroups, network, volumes
        Launch the container with isolation

    You get a running Nginx container accessible via port 80 (if mapped)


##What is dockershim?
🔹 dockershim is a translation layer that allowed Kubernetes to use Docker as a container runtime.

#Why was it needed?
Kubernetes doesn’t directly talk to Docker.
Instead, it uses a standard interface called the CRI (Container Runtime Interface).

  -->But Docker was not CRI-compatible by default, so Kubernetes introduced dockershim to bridge the gap.

-->
#In Simple Words:
    Kubernetes → talks CRI
    Docker → speaks its own API
    dockershim → sits in between, acting as a translator

     Kubernetes
       |
       | ← CRI
          ↓
    dockershim
       |
       | ← Docker API
          ↓
    Docker Engine (dockerd)
       |
          ↓
    containerd → runc → Container
    
    
##How It Worked in Container Creation
-->Kubernetes tells kubelet:
    “Create a Pod with this image.”
 
-->kubelet talks to dockershim via CRI.
   
-->dockershim translates the CRI request into Docker API calls.
   
-->dockerd creates the container using:
      --containerd (low-level manager)
      --runc (executes the container)
   
-->Container starts, and kubelet monitors it via dockershim.


##What is CRI (Container Runtime Interface)?
CRI stands for Container Runtime Interface.
It’s a standard API interface that allows the Kubernetes kubelet to communicate with different container runtimes, like:
    Docker (via dockershim)
    containerd (native CRI support)
    CRI-O
    gVisor or Kata Containers

#Purpose of CRI
    To decouple Kubernetes from being tied to a specific container runtime like Docker.
    Before CRI, Kubernetes had hardcoded support for Docker. CRI made it pluggable.

+-------------------+
|   kubelet         |
|   (asks to run a  |
|    Pod/Container) |
+--------+----------+
         |
         |  CRI (gRPC)
         v
+-------------------+
|   dockershim      |   ← Adapter
+--------+----------+
         |
         | Docker API (HTTP)
         v
+-------------------+
|   Docker Engine   |
+--------+----------+
         |
         v
+-------------------+
|  containerd + runc|
+-------------------+


##How It Works (Step-by-Step)
#Let’s say kubelet wants to create a Pod.

1)  kubelet sends a gRPC request via CRI.
        For example: RunPodSandbox() or CreateContainer()

2)  dockershim receives the CRI request.
        Translates it to Docker REST API (e.g., docker run)

3)  Docker Engine pulls the image, sets up cgroups/namespaces.

4)  Docker uses containerd internally to run the container using runc.

5)  dockershim collects logs, status, and sends it back to kubelet.


##Why dockershim Was Removed?
    It was extra overhead — more moving parts (Docker → containerd → runc)
    Docker itself relies on containerd internally.
    Kubernetes wanted direct CRI runtimes like:
        containerd
        CRI-O

Note- So, starting in Kubernetes v1.24 (2022), dockershim was deprecated and removed.

-->Does That Mean Docker Is Not Supported?
    ✅ Docker images still work — they follow the OCI standard.
    ❌ Docker runtime is no longer used directly in Kubernetes.
    ✅ containerd or CRI-O is used instead.


##What is a Pod?
Pod is the smallest deployable unit in Kubernetes.
    A Pod wraps one or more containers
    All containers in a Pod
        Share network namespace (same IP and port space)
        Share volumes
        Run on the same Node

#Why Pods?
Pods allow:
    Tightly coupled containers to work together (e.g., app + logging sidecar)
    Kubernetes to manage lifecycle, scaling, health, and networking


##Docker vs Pod (Side-by-Side Comparison)
Feature	               Docker Container	                         Kubernetes Pod
Scope	            Single container unit	           Group of one or more containers
Platform	      Docker Engine / CLI	              Kubernetes orchestration
Networking	   Has its own network namespace	  Shared IP across all containers
Volume Sharing	     Needs explicit setup	            Shared inside Pod automatically
Lifecycle	    Managed by Docker daemon	              Managed by Kubernetes
Scheduling	        Manual                                Kubernetes Scheduler


##How Docker & Pods Work in Kubernetes
Kubernetes used Docker as its default runtime earlier. Now it uses containerd, but the flow is similar.

#When You Deploy a Pod in K8s:
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: app
    image: my-nginx
    ports:
    - containerPort: 80

-->What Happens:
    kubectl sends request → kube-apiserver
    Control plane stores Pod spec in etcd
    kube-scheduler assigns a Node
    kubelet on Node:
        Pulls the container image via containerd
        Sets up namespace, volumes, network
        Launches container(s) in the Pod

   note: kube-proxy sets up routing so the Pod can be accessed
   
   
 --> Pod Without Docker?
Yes — Kubernetes now uses containerd or CRI-O instead of Docker directly because:
    Docker includes extra tools not needed for K8s
    Kubernetes only needs a lightweight container runtime interface (CRI)

Docker → containerd → runc
Kubernetes → containerd (direct)   
   
   
##Kubernetes components:
1)Kubectl:
  This request goes to the kube-apiserver, the central management gateway for the Kubernetes control plane.
  
2)kube-apiserver Receives the Request(API GATEWAY)
    Acts as a RESTful API server
    Validates your request (YAML syntax, resource definitions)
    Stores the desired state in etcd (the cluster’s database)
    Acts as the front door to the cluster.
    All components (users, internal services) talk to the API server.
    It receives REST requests, validates them, and updates the cluster state in etcd.

 -->Think of etcd as the source of truth for all cluster data: pods, nodes, secrets, config maps, etc.

example:
    kubectl create deployment nginx --image=nginx
    
  -- This goes to kube-apiserver.
  -- It stores the request (desired state) in etcd.


3)etcd Stores the Desired State(Cluster Database)
etcd stores:
    You want a Deployment named nginx-deploy
    It should have 3 replicas
    It should use the nginx:latest container image
    A distributed key-value store used to store all cluster data.
    Holds desired and actual state of objects (Pods, Services, ConfigMaps, etc.)
    Highly consistent and fault-tolerant.

example:
    When you create a Pod, its spec is stored in etcd.
    If a Node crashes, etcd still remembers what Pods should exist.


4)kube-controller-manager Reacts(State Watcher)
    Constantly watches etcd for desired vs. actual state
    Sees that 3 Pods are needed for nginx-deploy, but 0 exist
    Creates 3 Pod objects in etcd
    Runs many controllers that monitor etcd state and try to match it with the real world.
    If anything goes out of sync, it takes action to fix it.

#Key Controllers:
    ReplicaSet controller → Ensures desired # of Pods
    Node controller → Detects offline nodes
    Job controller → Ensures Jobs are completed

example:
   - You declare: “I want 3 replicas of nginx”
     If 1 crashes, controller-manager will recreate it to match the desired state.


5)kube-scheduler Picks Nodes
    Watches for new, unassigned Pods
    Chooses the best Node for each Pod based on:
        Resource availability (CPU/RAM)
        Node affinity rules
        Taints/tolerations 
     
    Updates the Pod object with the chosen Node name

example:
 -- A Pod is created → scheduler sees it’s unassigned
    It chooses Node-2 based on load → assigns the Pod

6)cloud-controller-manager (Optional, for Cloud Integration)
    Manages cloud-specific logic like:
        Load balancers
        Volume provisioning
        Node IP management (e.g., AWS, Azure, GCP)

Example:
    You define a Service of type LoadBalancer
    Cloud controller provisions a cloud LB and updates the service with its public IP


7)kubelet on Node Executes the Pod
    Each Node runs a kubelet agent
    kubelet watches the API server for Pods assigned to its node
    Runs on every Node
    Talks to kube-apiserver to get Pod specs assigned to that node
    Ensures containers are running as expected
    Monitors health, collects logs

    It:
        Pulls the required container images from registry
        Tells the container runtime (like containerd or Docker) to start the containers
        Monitors the health and status of those containers

  -->Kubelet regularly reports Pod status (running, failed, crashed) back to the API server.

example:
  -- A Pod is assigned to Node-1
     kubelet pulls its image, starts the container, reports status

8) kube-proxy Handles Networking
    Each Node runs kube-proxy
    Sets up iptables or IPVS rules to route traffic
    Exposes Kubernetes Services by:
        Creating virtual IPs (ClusterIP)
        Load-balancing traffic between Pods
        Forwarding traffic to correct container ports

example:
   -- A Service has 3 backend Pods
      kube-proxy load balances traffic across them

9)Container Runtime (Runs Containers)
    Software that actually runs containers (e.g., containerd, CRI-O)
    Interfaced via CRI (Container Runtime Interface)

Example:
    When kubelet tells the runtime to "run container nginx:latest", it:
        Pulls image from registry
        Sets up network, cgroups, namespace
        Starts the container process

10) Service and Ingress for Access
    A Service gives Pods a stable internal IP address and DNS name.
    An Ingress Controller can route external HTTP(S) traffic to Services using URLs or hostnames.
    Internally, Kubernetes uses CoreDNS to resolve pod and service names.


###HOW THEY WORK TOGETHER — END-TO-END EXAMPLE
Let’s say you run:

kubectl create deployment nginx --image=nginx --replicas=3

Here’s what happens behind the scenes:
  -->kubectl → API Server
        Sends a POST request to kube-apiserver
        Deployment is created, stored in etcd

  -->Controller Manager sees:
        3 Pods are desired but none exist
        It creates 3 Pod objects in etcd

  -->Scheduler:
        Notices these Pods are unscheduled
        Assigns each to a Node

  -->kubelet on each Node:
        Sees that it’s supposed to run a Pod
        Pulls the image via container runtime
        Starts the container

  -->kube-proxy:
        Sets up networking rules
        Ensures requests to the Service go to the correct Pods

    You can access the app via Service IP or Ingress.

  -->If a Pod crashes:
        Controller manager notices it
        Re-creates the Pod to maintain desired count



##Summary of Component Communication
Component	      Talks to	                             Purpose
kubectl	             kube-apiserver	                Submit requests
kube-apiserver	     etcd, all other	                Cluster gateway, validates/forwards all ops
etcd	             Only kube-apiserver	        Stores persistent cluster state
controller-manager   kube-apiserver	                Reconciles state, creates pods, nodes, etc.
scheduler	     kube-apiserver	                Assigns pods to nodes
kubelet	             kube-apiserver	                Runs containers, reports status
kube-proxy	     Node network stack              	Manages internal cluster networking
container runtime    kubelet	                        Starts/stops containers

##architecture
User (kubectl)
   |
   v
kube-apiserver <--> etcd
   |
   |--> kube-controller-manager: Create 3 Pods
   |
   |--> kube-scheduler: Assign Nodes
   |
   |--> kubelet on Node A: Start Pod 1
   |--> kubelet on Node B: Start Pod 2
   |--> kubelet on Node C: Start Pod 3
   |
   |--> kube-proxy on each Node: Route traffic to Pods
   
   
## Key Communication Protocols
Component                Communication	                                   Protocol/Port
kubectl                  kube-apiserver	                                    HTTPS (TCP 6443)
kube-apiserver           etcd	                                            HTTPS (TCP 2379)
kubelet                  container runtime	                            CRI (gRPC over Unix)
kubelet                  kube-apiserver	                                    HTTPS (TCP 6443)
kube-proxy               API server	                                    HTTPS (TCP 6443)
Pod                      Pod (same Node)	                            Direct via container network
Pod                      Pod (across Nodes)	                            Overlay network (CNI plugin like Calico, Flannel, etc.)


-->Monitoring and Logs
    kubelet exposes metrics (on port 10255/10250)
    You can use Prometheus + Grafana to scrape metrics
    Fluentd, Loki, or ELK stack for logs



