###############CEPH INSTALLATION WITH REEF#############

##step-1###
check to /etc/hosts File on All Nodes.
Edit the /etc/hosts file:

###step 2#####
update and upgrade system packages
sudo apt-get update && sudo apt-get upgrade -y

#########create cephadmin username accordingly#########
useradd -m -s /bin/bash cephadmin
passwd cephadmin
echo "cephadmin ALL=(ALL:ALL) NOPASSWD:ALL" >> /etc/sudoers.d/cephadmin
chmod 0440 /etc/sudoers.d/cephadmin

###########Install Docker on Each ceph nodes on  ubuntu ##########################3

sudo apt install apt-transport-https ca-certificates curl software-properties-common -y
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt -y update
sudo apt -y install docker-ce
sudo systemctl enable --now docker

##############Ceph initial deploy on (cephadm) node1    ##############
curl -fsSL https://download.ceph.com/keys/release.asc | sudo gpg --dearmor -o /usr/share/keyrings/ceph-release.gpg
echo "deb [signed-by=/usr/share/keyrings/ceph-release.gpg] https://download.ceph.com/debian-reef/ $(lsb_release -sc) main" | sudo tee /etc/apt/sources.list.d/ceph.list
sudo apt-get update
apt-cache policy ceph
curl --silent --remote-name --location https://github.com/ceph/ceph/raw/18.2.7/src/cephadm/cephadm

chmod +x cephadm
apt -y update && apt -y upgrade
apt -y install cephadm
cephadm version

cephadm add-repo --release reef 
sudo cephadm install

sudo apt install ceph-common 

############## bootstrap mon host #####################33

sudo cephadm bootstrap --mon-ip  10.10.50.104 --initial-dashboard-user admin  --initial-dashboard-password admin098! --dashboard-password-noupdate

##############Copy keys to root user to other cephs
ssh-keygen
cat .ssh/id_rsa.pub
vi .ssh/authorized_keys


ssh-copy-id -f -i /etc/ceph/ceph.pub root@cephd2
ssh-copy-id -f -i /etc/ceph/ceph.pub root@cephd4

###############Ceph cephs add via orch(orchestrator)

sudo ceph orch host add GFBM2 10.121.223.22
sudo ceph orch host add GFBM3 10.121.223.23


####################Set public network for our ceph cluster ############333

sudo ceph config set mon public_network 10.138.230.0/23

###################Deploy monitors on a specific set of hosts ###############3
ceph orch host ls
sudo ceph orch apply mon "GFBM1,GFBM2,GFBM3"
 
########## ceph add lable ############
sudo ceph orch host label add GFBM1 mon
sudo ceph orch host label add GFBM2 mon
sudo ceph orch host label add GFBM3 mon

ceph orch host ls

###############ceph list hosts ##############

sudo ceph osd tree | column -t -s $'\t'

######### add osd host###############3
ceph orch device ls

################## create osd from a specific device on a specific host ######################################3
###create a file ceph1_osd.yml####
service_type: osd
service_id: seemoosp_osd
service_name: osd.seemoosp_osd
placement:
  host_pattern: "seemoosp"
spec:
  data_devices:
    paths:
    - /dev/sda
    - /dev/sdb
  unmanaged: False
    
ceph orch apply osd -i osd_ceph1.yaml
ceph orch apply osd -i osd_ceph2.yaml

######## DEPLOY RGWS service #####################
ceph orch host label add ceph1 rgw
ceph orch host label add ceph2 rgw
ceph orch apply rgw radosGW '--placement=label:rgw count-per-host:1' --port=8080

################# rgw user list and info  #########3
radosgw-admin user list
radosgw-admin user info --uid=dashboard

############# mgr set up in all node
ceph orch apply mgr --placement=" GFBM1 GFBM2 GFBM3 "
ceph balancer on

###################all node install################
 apt install -y chrony && apt install -y s3cmd
 

#########command to set the osd pool size replica1  #########
for pool in $(ceph osd pool ls); do
 ceph osd pool set "$pool" size 1
done


#############mannual commands to set pool size###########
ceph config set mon mon_allow_pool_size_one true
ceph config set mon mon_allow_pool_delete true
ceph config get mon mon_allow_pool_delete

#############delete osd pools############### 
ceph osd pool delete .rgw.root .rgw.root --yes-i-really-really-mean-it
ceph osd pool delete .mgr .mgr --yes-i-really-really-mean-it
ceph osd pool delete default.rgw.log default.rgw.log --yes-i-really-really-mean-it
ceph osd pool delete default.rgw.control default.rgw.control --yes-i-really-really-mean-it
ceph osd pool delete default.rgw.meta default.rgw.meta --yes-i-really-really-mean-it
ceph osd pool delete default.rgw.buckets.index default.rgw.buckets.index --yes-i-really-really-mean-it
ceph osd pool delete volumes volumes --yes-i-really-really-mean-it
ceph osd pool delete images images --yes-i-really-really-mean-it
ceph osd pool delete vms vms --yes-i-really-really-mean-it
ceph osd pool delete gnocchi gnocchi --yes-i-really-really-mean-it
ceph osd pool delete backup backup --yes-i-really-really-mean-it

###########create osd pools#################
ceph osd pool create rgw.root 32
ceph osd pool create mgr 32
ceph osd pool create default.rgw.log 32
ceph osd pool create default.rgw.control 32
ceph osd pool create default.rgw.meta 32
ceph osd pool create default.rgw.buckets.index 32
ceph osd pool create volumes 32
ceph osd pool create images 32
ceph osd pool create vms 32
ceph osd pool create gnocchi 32
ceph osd pool create backup 32

########set replica size 1#############
ceph osd pool set mgr size 1 --yes-i-really-mean-it
ceph osd pool set default.rgw.log size 1 --yes-i-really-mean-it
ceph osd pool set default.rgw.control size 1 --yes-i-really-mean-it
ceph osd pool set default.rgw.meta size 1 --yes-i-really-mean-it
ceph osd pool set default.rgw.buckets.index size 1 --yes-i-really-mean-it
 
#########enable pools################
ceph osd pool application enable .rgw.root rgw
ceph osd pool application enable default.rgw.log rgw
ceph osd pool application enable default.rgw.control rgw
ceph osd pool application enable default.rgw.meta rgw
ceph osd pool application enable default.rgw.buckets.index rgw
ceph osd pool application enable mgr rgw
ceph osd pool application enable volumes rbd
ceph osd pool application enable images rbd
ceph osd pool application enable vms rbd
ceph osd pool application enable gnocchi rbd
ceph osd pool application enable backup rbd


#########if docker error or container not create#####
sudo systemctl stop docker
sudo mv /var/lib/docker /var/lib/docker.bak
sudo systemctl start docker
sudo rm -rf /var/lib/docker/overlay2
sudo systemctl restart docker
reboot

